{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.models import Sequential, Layer\n",
    "from keras.layers import Dense, MaxPool2D, Dropout,MaxPool2D, Activation,BatchNormalization, InputLayer, Embedding, Flatten, Conv2D,concatenate, MaxPooling2D, Input, TimeDistributed\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import keras.utils\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Flatten, LSTM\n",
    "import numpy as np\n",
    "from keras import objectives\n",
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "(6, 10, 64, 64, 3)\n",
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "s1 = np.random.rand(10,64,64,3).reshape(-1,64,64,3)\n",
    "s2 = np.random.rand(10,64,64,3).reshape(-1,64,64,3)\n",
    "s3 = np.random.rand(10,64,64,3).reshape(-1,64,64,3)\n",
    "s4 = np.random.rand(10,64,64,3).reshape(-1,64,64,3)\n",
    "s5 = np.random.rand(10,64,64,3).reshape(-1,64,64,3)\n",
    "s6 = np.random.rand(10,64,64,3).reshape(-1,64,64,3)\n",
    "data = np.array([s1,s2,s3,s4,s5,s6]).reshape(-1,10,64,64,3)\n",
    "label = np.array([0,1,2,3,4,5]).reshape(-1)\n",
    "print(s1.shape)\n",
    "print(data.shape)\n",
    "print(label.shape)\n",
    "label = keras.utils.to_categorical(label, num_classes=None)\n",
    "K.set_image_data_format('channels_last')\n",
    "K.image_data_format()\n",
    "K.set_learning_phase(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def min_max_norm(cossim):\n",
    "    min_m = K.min(cossim)\n",
    "    max_m = K.max(cossim)\n",
    "    normalized =[((x-min_m)/(max_m-min_m)) for x in cossim];\n",
    "    print(\"success:: normalized\")\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossy(x_mat):\n",
    "    nm_x = [K.l2_normalize(x_mat[i][:])  for i in range(10)]\n",
    "    apex_vec = nm_x[-1]\n",
    "    cossim = [K.dot(vec, apex_vec) for vec in nm_x]\n",
    "    print(\"success:: cossim\")\n",
    "    return cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossimilasrity(x):\n",
    "    cossim = cossy(x)\n",
    "    normalized =min_max_norm(cossim)\n",
    "    print(\"success:: cossimilarity\")\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_func(y_true, y_pred):\n",
    "    \n",
    "    print(\"Begin:: loss_func\")\n",
    "    \n",
    "    y_true = cossimilasrity(y_true)\n",
    "    y_pred = cossimilasrity(y_pred)\n",
    "    \n",
    "    y_true = K.concatenate(y_true, axis=-1)\n",
    "    y_pred = K.concatenate(y_pred, axis=-1)\n",
    "    \n",
    "    loss = objectives.mean_squared_error(y_true, y_pred)\n",
    "    print(\"success:: loss_func\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c2595d68>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c2594f60>\n",
      "Deactivate::  <keras.layers.normalization.BatchNormalization object at 0x1c2595fd0>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c25d69e8>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c25ffe10>\n",
      "Deactivate::  <keras.layers.normalization.BatchNormalization object at 0x1c25eeeb8>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c2690a58>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c26b91d0>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c270eb38>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c58b1240>\n",
      "Deactivate::  <keras.layers.normalization.BatchNormalization object at 0x1c589eac8>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c58e9fd0>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c5910940>\n",
      "Deactivate::  <keras.layers.normalization.BatchNormalization object at 0x1c5910c50>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c59a1a20>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c59cb668>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c59e0198>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c5a94160>\n",
      "Begin:: loss_func\n",
      "success:: cossim\n",
      "success:: normalized\n",
      "success:: cossimilarity\n",
      "success:: cossim\n",
      "success:: normalized\n",
      "success:: cossimilarity\n",
      "success:: loss_func\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c2595d68>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c2594f60>\n",
      "Deactivate::  <keras.layers.normalization.BatchNormalization object at 0x1c2595fd0>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c25d69e8>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c25ffe10>\n",
      "Deactivate::  <keras.layers.normalization.BatchNormalization object at 0x1c25eeeb8>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c2690a58>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c26b91d0>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c270eb38>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c58b1240>\n",
      "Deactivate::  <keras.layers.normalization.BatchNormalization object at 0x1c589eac8>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c58e9fd0>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c5910940>\n",
      "Deactivate::  <keras.layers.normalization.BatchNormalization object at 0x1c5910c50>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c59a1a20>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c59cb668>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c59e0198>\n",
      "Deactivate::  <keras.layers.wrappers.TimeDistributed object at 0x1c5a94160>\n",
      "Deactivate::  <keras.layers.recurrent.LSTM object at 0x1c5a94710>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_182 (TimeDi (None, 10, 60, 60, 64)    4864      \n",
      "_________________________________________________________________\n",
      "time_distributed_183 (TimeDi (None, 10, 60, 60, 64)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 10, 60, 60, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_184 (TimeDi (None, 10, 60, 60, 64)    36928     \n",
      "_________________________________________________________________\n",
      "time_distributed_185 (TimeDi (None, 10, 60, 60, 64)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 10, 60, 60, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_186 (TimeDi (None, 10, 30, 30, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_187 (TimeDi (None, 10, 30, 30, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_188 (TimeDi (None, 10, 30, 30, 32)    18464     \n",
      "_________________________________________________________________\n",
      "time_distributed_189 (TimeDi (None, 10, 30, 30, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_75 (Batc (None, 10, 30, 30, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_190 (TimeDi (None, 10, 30, 30, 32)    9248      \n",
      "_________________________________________________________________\n",
      "time_distributed_191 (TimeDi (None, 10, 30, 30, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_76 (Batc (None, 10, 30, 30, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_192 (TimeDi (None, 10, 15, 15, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_193 (TimeDi (None, 10, 15, 15, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_194 (TimeDi (None, 10, 7200)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_195 (TimeDi (None, 10, 512)           3686912   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 6)                 12456     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 6)                 42        \n",
      "=================================================================\n",
      "Total params: 5,868,882\n",
      "Trainable params: 3,768,914\n",
      "Non-trainable params: 2,099,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(64, 5, data_format=\"channels_last\", kernel_initializer=\"he_normal\", name = 'c1'), input_shape=s1.shape))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3),  padding='same', name = 'c2')))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3),padding='same', name = 'c3')))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3),padding='same', name = 'c4')))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(TimeDistributed(Dense(512)))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "for layer in model.layers[:18]:\n",
    "    print(\"Deactivate:: \" , layer)\n",
    "    layer.trainable = False\n",
    "    \n",
    "for layer in model.layers[18:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.add(LSTM(512, return_sequences=True, dropout=0.2, input_shape = (10,512),name='lstm_1' ))#E2 Objective Covered\n",
    "model.compile(loss= loss_func, optimizer = 'adam', metrics=['accuracy'])\n",
    "\n",
    "for layer in model.layers[:19]:\n",
    "    print(\"Deactivate:: \" , layer)\n",
    "    layer.trainable = False\n",
    "    \n",
    "for layer in model.layers[19:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.add(LSTM(6, return_sequences=False, dropout=0.2, input_shape = (None,10,512), name='lstm_2'))#E1 Objective Covered\n",
    "model.add(Dense(6,  activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])#'categorical_crossentropy'\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 [==============================] - 14s 2s/step - loss: 2.0725 - acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.7781 - acc: 0.3333\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.7645 - acc: 0.3333\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 8s 1s/step - loss: 1.8648 - acc: 0.3333\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 8s 1s/step - loss: 1.7754 - acc: 0.1667\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 7s 1s/step - loss: 1.7398 - acc: 0.3333\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.9395 - acc: 0.1667\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.8356 - acc: 0.3333\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 6s 972ms/step - loss: 1.8015 - acc: 0.3333\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.7896 - acc: 0.1667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c6a2c160>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, label, batch_size=1, epochs = 10)\n",
    "# score, acc = model.evaluate(data, label)\n",
    "# print('Test score:', score)\n",
    "# print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_32 (Conv2D)           (None, 60, 60, 64)        4864      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 60, 60, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 57, 57, 64)        65600     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 57, 57, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 26, 26, 32)        18464     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               2359808   \n",
      "=================================================================\n",
      "Total params: 2,458,112\n",
      "Trainable params: 2,458,048\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#cnn mian seperate import\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(Conv2D(64, 5, data_format=\"channels_last\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape=(64, 64, 3)))\n",
    "#cnnmodel.add(BatchNormalization())\n",
    "cnnmodel.add(Activation(\"relu\"))\n",
    "\n",
    "cnnmodel.add(Conv2D(64, 4))\n",
    "#cnnmodel.add(BatchNormalization())\n",
    "cnnmodel.add(Activation(\"relu\"))\n",
    "cnnmodel.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "cnnmodel.add(Dropout(0.5))\n",
    "\n",
    "cnnmodel.add(Conv2D(32, 3))\n",
    "#cnnmodel.add(BatchNormalization())\n",
    "cnnmodel.add(Activation(\"relu\"))\n",
    "\n",
    "cnnmodel.add(Conv2D(32, 3))\n",
    "cnnmodel.add(keras.layers.normalization.BatchNormalization())#BatchNormalization())\n",
    "cnnmodel.add(Activation(\"relu\"))\n",
    "\n",
    "cnnmodel.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "cnnmodel.add(Dropout(0.5))\n",
    "\n",
    "cnnmodel.add(Flatten())\n",
    "cnnmodel.add(Dense(512))\n",
    "cnnmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnnmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_8 (TimeDist (None, 1000, 60, 60, 64)  4864      \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 1000, 58, 58, 32)  18464     \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 1000, 29, 29, 32)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 1000, 29, 29, 64)  18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 1000, 29, 29, 64)  36928     \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 1000, 14, 14, 64)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 1000, 14, 14, 128) 73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 1000, 14, 14, 128) 147584    \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 1000, 7, 7, 128)   0         \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 1000, 7, 7, 256)   295168    \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 1000, 7, 7, 256)   590080    \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 1000, 3, 3, 256)   0         \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 1000, 3, 3, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 1000, 3, 3, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "time_distributed_22 (TimeDis (None, 1000, 1, 1, 512)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1000, 1, 1, 512)   2048      \n",
      "_________________________________________________________________\n",
      "time_distributed_23 (TimeDis (None, 1000, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000, 512)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1000, 256)         787456    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000, 6)           1542      \n",
      "=================================================================\n",
      "Total params: 5,516,454\n",
      "Trainable params: 5,515,430\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## approach 2\n",
    "model = Sequential()\n",
    "sequence = Input(shape=[None, 64, 64, 3])\n",
    "model.add(TimeDistributed(Conv2D(64, 5, data_format=\"channels_last\", kernel_initializer=\"he_normal\"), input_shape=(1000, 64, 64, 3)))\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3),  padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(128, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(128, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(256, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(256, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(512, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(512, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(256, return_sequences=True, dropout=0.5))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
