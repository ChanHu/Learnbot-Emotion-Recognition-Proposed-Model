{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.models import Sequential, Layer\n",
    "from keras.layers import Dense, MaxPool2D, Dropout,MaxPool2D, Activation,BatchNormalization, InputLayer, Embedding, Flatten, Conv2D,concatenate, MaxPooling2D, Input, TimeDistributed\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import keras.utils\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Flatten, LSTM\n",
    "import numpy as np\n",
    "from keras import objectives\n",
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64, 64, 3)\n",
      "(6, 1, 64, 64, 3)\n",
      "(6,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'channels_last'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = np.random.rand(1,64,64,3).reshape(-1,64,64,3)\n",
    "s2 = np.random.rand(1,64,64,3).reshape(-1,64,64,3)\n",
    "s3 = np.random.rand(1,64,64,3).reshape(-1,64,64,3)\n",
    "s4 = np.random.rand(1,64,64,3).reshape(-1,64,64,3)\n",
    "s5 = np.random.rand(1,64,64,3).reshape(-1,64,64,3)\n",
    "s6 = np.random.rand(1,64,64,3).reshape(-1,64,64,3)\n",
    "data = np.array([s1,s2,s3,s4,s5,s6]).reshape(-1,1,64,64,3)\n",
    "label = np.array([0,1,2,3,4,5]).reshape(-1)\n",
    "print(s1.shape)\n",
    "print(data.shape)\n",
    "print(label.shape)\n",
    "label = keras.utils.to_categorical(label, num_classes=None)\n",
    "K.set_image_data_format('channels_last')\n",
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-d7ed327fc512>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-d7ed327fc512>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    lay = AnyKerasLayer(params...)(inLay)\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def createLossModel():\n",
    "\n",
    "    inLay = Input(#shape of your original output)\n",
    "\n",
    "    #use your lambdas here.....\n",
    "    lay = AnyKerasLayer(params...)(inLay)\n",
    "    lay = AnyOtherKerasLayer(params...)(lay)\n",
    "    ...\n",
    "    output = OneMoreLayer(params...)(lay)\n",
    "\n",
    "    m = Model(inLay,output)\n",
    "\n",
    "    #it's important to make this model not trainable if it has weights \n",
    "        #(you should probably set these weights manually if that's the case)    \n",
    "    m.trainable = False\n",
    "    for l in m.layers: l.trainable = False\n",
    "     \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRegularization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomRegularization, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self ,x ,mask=None):\n",
    "        x = cossimilasrity(x)#[Nf,1]\n",
    "        obejective.mean_squared_error(y_true, y_pred)\n",
    "        return bce\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        print(input_shape)\n",
    "        return ((None, 1, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zero_loss(y_true, y_pred):\n",
    "    return K.zeros_like(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(y_true, y_pred):\n",
    "    print(\"loss_func\")\n",
    "    print(y_true)\n",
    "    print(y_pred)\n",
    "    return K.mean((y_true - y_pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##E2 formulate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(64, 5, data_format=\"channels_last\", kernel_initializer=\"he_normal\", activation='relu'), input_shape=s1.shape))\n",
    "#model.add(TimeDistributed())\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3),  padding='same', activation='relu')))\n",
    "# model.add(TimeDistributed())\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3),padding='same',activation='relu')))\n",
    "# model.add(TimeDistributed())\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3),padding='same',activation='relu')))\n",
    "# model.add(TimeDistributed())\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(TimeDistributed(Dense(512)))\n",
    "#model.compile()\n",
    "# freeze\n",
    "model.add(LSTM(512, return_sequences=True, dropout=0.2))\n",
    "# model.add(CustomRegularization())\n",
    "model.compile(loss= loss_func, optimizer = 'adam', metrics=['accuracy'])\n",
    "#freeze\n",
    "# model.add()\n",
    "model.add(LSTM(6, return_sequences=False, dropout=0.2))#E1 comp\n",
    "#model.add(Activation('softmax'))\n",
    "# model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])#'categorical_crossentropy'\n",
    "#compile E3 layerlstm\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 4s 588ms/step - loss: 0.1444 - acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 254ms/step - loss: 0.1398 - acc: 0.1667\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.1404 - acc: 0.1667\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.1389 - acc: 0.1667\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 239ms/step - loss: 0.1383 - acc: 0.1667\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 291ms/step - loss: 0.1388 - acc: 0.1667\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 3s 421ms/step - loss: 0.1391 - acc: 0.1667\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 340ms/step - loss: 0.1383 - acc: 0.1667\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 376ms/step - loss: 0.1399 - acc: 0.1667\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 308ms/step - loss: 0.1394 - acc: 0.1667\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.1395 - acc: 0.1667\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.1394 - acc: 0.1667\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.1384 - acc: 0.1667\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.1393 - acc: 0.1667\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 247ms/step - loss: 0.1392 - acc: 0.1667\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 272ms/step - loss: 0.1386 - acc: 0.1667\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.1384 - acc: 0.1667\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 0.1390 - acc: 0.1667\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 274ms/step - loss: 0.1399 - acc: 0.1667\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.1402 - acc: 0.1667\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 233ms/step - loss: 0.1403 - acc: 0.1667\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.1390 - acc: 0.1667\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 2s 307ms/step - loss: 0.1402 - acc: 0.1667\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 243ms/step - loss: 0.1397 - acc: 0.1667\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 2s 287ms/step - loss: 0.1395 - acc: 0.1667\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 245ms/step - loss: 0.1397 - acc: 0.1667\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.1399 - acc: 0.1667\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 2s 298ms/step - loss: 0.1393 - acc: 0.1667\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 2s 270ms/step - loss: 0.1401 - acc: 0.1667\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.1404 - acc: 0.1667\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 2s 347ms/step - loss: 0.1389 - acc: 0.1667\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 242ms/step - loss: 0.1392 - acc: 0.1667\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.1395 - acc: 0.1667\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 2s 270ms/step - loss: 0.1398 - acc: 0.1667\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 222ms/step - loss: 0.1388 - acc: 0.1667\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 249ms/step - loss: 0.1384 - acc: 0.1667\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.1401 - acc: 0.1667\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 2s 303ms/step - loss: 0.1378 - acc: 0.1667\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 2s 282ms/step - loss: 0.1379 - acc: 0.3333\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 2s 297ms/step - loss: 0.1396 - acc: 0.1667\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 2s 298ms/step - loss: 0.1402 - acc: 0.1667\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 2s 252ms/step - loss: 0.1390 - acc: 0.1667\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 2s 345ms/step - loss: 0.1401 - acc: 0.1667\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 2s 290ms/step - loss: 0.1398 - acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.1386 - acc: 0.3333\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 243ms/step - loss: 0.1382 - acc: 0.3333\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.1395 - acc: 0.1667\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 2s 250ms/step - loss: 0.1395 - acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 2s 299ms/step - loss: 0.1392 - acc: 0.1667\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.1397 - acc: 0.1667\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.1398 - acc: 0.3333\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 222ms/step - loss: 0.1400 - acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 239ms/step - loss: 0.1389 - acc: 0.1667\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 0.1388 - acc: 0.1667\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 2s 284ms/step - loss: 0.1390 - acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.1386 - acc: 0.5000\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 225ms/step - loss: 0.1399 - acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 224ms/step - loss: 0.1387 - acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 0.1381 - acc: 0.1667\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 221ms/step - loss: 0.1370 - acc: 0.6667\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 224ms/step - loss: 0.1390 - acc: 0.1667\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.1387 - acc: 0.3333\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 0.1385 - acc: 0.1667\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 2s 254ms/step - loss: 0.1390 - acc: 0.3333\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 237ms/step - loss: 0.1386 - acc: 0.3333\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.1401 - acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 2s 271ms/step - loss: 0.1406 - acc: 0.1667\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 241ms/step - loss: 0.1381 - acc: 0.3333\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 200ms/step - loss: 0.1398 - acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 222ms/step - loss: 0.1365 - acc: 0.5000\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 2s 274ms/step - loss: 0.1380 - acc: 0.3333\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.1416 - acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.1404 - acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 2s 298ms/step - loss: 0.1390 - acc: 0.1667\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 2s 308ms/step - loss: 0.1402 - acc: 0.1667\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 2s 250ms/step - loss: 0.1390 - acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 2s 282ms/step - loss: 0.1399 - acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.1407 - acc: 0.1667\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 2s 294ms/step - loss: 0.1382 - acc: 0.1667\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.1398 - acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.1386 - acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 2s 285ms/step - loss: 0.1395 - acc: 0.1667\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.1400 - acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.1396 - acc: 0.1667\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 2s 283ms/step - loss: 0.1390 - acc: 0.1667\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 237ms/step - loss: 0.1394 - acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 2s 366ms/step - loss: 0.1387 - acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 239ms/step - loss: 0.1392 - acc: 0.1667\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 2s 252ms/step - loss: 0.1390 - acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 2s 281ms/step - loss: 0.1387 - acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 205ms/step - loss: 0.1390 - acc: 0.1667\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 241ms/step - loss: 0.1394 - acc: 0.1667\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 2s 296ms/step - loss: 0.1383 - acc: 0.3333\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.1389 - acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.1395 - acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.1384 - acc: 0.3333\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 236ms/step - loss: 0.1404 - acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 2s 274ms/step - loss: 0.1396 - acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 218ms/step - loss: 0.1382 - acc: 0.1667\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 2s 251ms/step - loss: 0.1399 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122be8940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, label, batch_size=1, epochs = 100)\n",
    "# score, acc = model.evaluate(data, label)\n",
    "# print('Test score:', score)\n",
    "# print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_32 (Conv2D)           (None, 60, 60, 64)        4864      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 60, 60, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 57, 57, 64)        65600     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 57, 57, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 26, 26, 32)        18464     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               2359808   \n",
      "=================================================================\n",
      "Total params: 2,458,112\n",
      "Trainable params: 2,458,048\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#mian\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(Conv2D(64, 5, data_format=\"channels_last\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape=(64, 64, 3)))\n",
    "#cnnmodel.add(BatchNormalization())\n",
    "cnnmodel.add(Activation(\"relu\"))\n",
    "\n",
    "cnnmodel.add(Conv2D(64, 4))\n",
    "#cnnmodel.add(BatchNormalization())\n",
    "cnnmodel.add(Activation(\"relu\"))\n",
    "cnnmodel.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "cnnmodel.add(Dropout(0.5))\n",
    "\n",
    "cnnmodel.add(Conv2D(32, 3))\n",
    "#cnnmodel.add(BatchNormalization())\n",
    "cnnmodel.add(Activation(\"relu\"))\n",
    "\n",
    "cnnmodel.add(Conv2D(32, 3))\n",
    "cnnmodel.add(keras.layers.normalization.BatchNormalization())#BatchNormalization())\n",
    "cnnmodel.add(Activation(\"relu\"))\n",
    "\n",
    "cnnmodel.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "cnnmodel.add(Dropout(0.5))\n",
    "\n",
    "cnnmodel.add(Flatten())\n",
    "cnnmodel.add(Dense(512))\n",
    "cnnmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnnmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_8 (TimeDist (None, 1000, 60, 60, 64)  4864      \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 1000, 58, 58, 32)  18464     \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 1000, 29, 29, 32)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 1000, 29, 29, 64)  18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 1000, 29, 29, 64)  36928     \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 1000, 14, 14, 64)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 1000, 14, 14, 128) 73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 1000, 14, 14, 128) 147584    \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 1000, 7, 7, 128)   0         \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 1000, 7, 7, 256)   295168    \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 1000, 7, 7, 256)   590080    \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 1000, 3, 3, 256)   0         \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 1000, 3, 3, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 1000, 3, 3, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "time_distributed_22 (TimeDis (None, 1000, 1, 1, 512)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1000, 1, 1, 512)   2048      \n",
      "_________________________________________________________________\n",
      "time_distributed_23 (TimeDis (None, 1000, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000, 512)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1000, 256)         787456    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000, 6)           1542      \n",
      "=================================================================\n",
      "Total params: 5,516,454\n",
      "Trainable params: 5,515,430\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "sequence = Input(shape=[None, 64, 64, 3])\n",
    "model.add(TimeDistributed(Conv2D(64, 5, data_format=\"channels_last\", kernel_initializer=\"he_normal\"), input_shape=(1000, 64, 64, 3)))\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3),  padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(128, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(128, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(256, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(256, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(512, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(512, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(256, return_sequences=True, dropout=0.5))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable batch_normalization_1/moving_mean/biased already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/Users/shreyashkawalkar/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 1001, in moving_average_update\n    x, value, momentum, zero_debias=True)\n  File \"/Users/shreyashkawalkar/anaconda3/lib/python3.6/site-packages/keras/layers/normalization.py\", line 185, in call\n    self.momentum),\n  File \"/Users/shreyashkawalkar/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6de0a3c3cd7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnnmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_uid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# (num_samples * timesteps, ...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_uses_learning_phase'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0muses_learning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m   2076\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_tensor_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2078\u001b[0;31m             \u001b[0moutput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2079\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mrun_internal_graph\u001b[0;34m(self, inputs, masks)\u001b[0m\n\u001b[1;32m   2227\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0;34m'mask'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m                                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputed_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2229\u001b[0;31m                             \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2230\u001b[0m                             output_masks = _to_list(layer.compute_mask(computed_tensor,\n\u001b[1;32m   2231\u001b[0m                                                                        computed_mask))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/normalization.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    183\u001b[0m         self.add_update([K.moving_average_update(self.moving_mean,\n\u001b[1;32m    184\u001b[0m                                                  \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                                                  self.momentum),\n\u001b[0m\u001b[1;32m    186\u001b[0m                          K.moving_average_update(self.moving_variance,\n\u001b[1;32m    187\u001b[0m                                                  \u001b[0mvariance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mmoving_average_update\u001b[0;34m(x, value, momentum)\u001b[0m\n\u001b[1;32m    999\u001b[0m     \"\"\"\n\u001b[1;32m   1000\u001b[0m     return moving_averages.assign_moving_average(\n\u001b[0;32m-> 1001\u001b[0;31m         x, value, momentum, zero_debias=True)\n\u001b[0m\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/moving_averages.py\u001b[0m in \u001b[0;36massign_moving_average\u001b[0;34m(variable, value, decay, zero_debias, name)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mdecay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mzero_debias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mupdate_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_zero_debias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mupdate_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/moving_averages.py\u001b[0m in \u001b[0;36m_zero_debias\u001b[0;34m(unbiased_var, value, decay)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mlocal_step_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m       biased_var = variable_scope.get_variable(\n\u001b[0;32m--> 180\u001b[0;31m           \"biased\", initializer=biased_initializer, trainable=False)\n\u001b[0m\u001b[1;32m    181\u001b[0m       local_step = variable_scope.get_variable(\n\u001b[1;32m    182\u001b[0m           \u001b[0;34m\"local_step\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1047\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1050\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1051\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    946\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    651\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 653\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    654\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable batch_normalization_1/moving_mean/biased already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/Users/shreyashkawalkar/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 1001, in moving_average_update\n    x, value, momentum, zero_debias=True)\n  File \"/Users/shreyashkawalkar/anaconda3/lib/python3.6/site-packages/keras/layers/normalization.py\", line 185, in call\n    self.momentum),\n  File \"/Users/shreyashkawalkar/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "sequence = Input(shape=[1000,256, 256, 3])\n",
    "model.add(TimeDistributed(cnnmodel,input_shape=(1000,64,64,3)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnnmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4b425255f86a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnnmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnnmodel' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential() \n",
    "sequence = Input(shape=[None, 64, 64, 3])\n",
    "model.add(TimeDistributed(TimeDistributed(cnnmodel)),(sequence))\n",
    "model.add(LSTM(256, return_sequences=False, dropout=0.5))\n",
    "model.add(Dense(self.nb_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable batch_normalization_1/moving_mean/biased already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/Users/shreyashkawalkar/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 1001, in moving_average_update\n    x, value, momentum, zero_debias=True)\n  File \"/Users/shreyashkawalkar/anaconda3/lib/python3.6/site-packages/keras/layers/normalization.py\", line 185, in call\n    self.momentum),\n  File \"/Users/shreyashkawalkar/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-397c03a49866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnnmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdense_512_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_512_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_uid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# (num_samples * timesteps, ...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_uses_learning_phase'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0muses_learning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m   2076\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_tensor_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2078\u001b[0;31m             \u001b[0moutput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2079\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mrun_internal_graph\u001b[0;34m(self, inputs, masks)\u001b[0m\n\u001b[1;32m   2227\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0;34m'mask'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m                                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputed_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2229\u001b[0;31m                             \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2230\u001b[0m                             output_masks = _to_list(layer.compute_mask(computed_tensor,\n\u001b[1;32m   2231\u001b[0m                                                                        computed_mask))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/normalization.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    183\u001b[0m         self.add_update([K.moving_average_update(self.moving_mean,\n\u001b[1;32m    184\u001b[0m                                                  \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                                                  self.momentum),\n\u001b[0m\u001b[1;32m    186\u001b[0m                          K.moving_average_update(self.moving_variance,\n\u001b[1;32m    187\u001b[0m                                                  \u001b[0mvariance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mmoving_average_update\u001b[0;34m(x, value, momentum)\u001b[0m\n\u001b[1;32m    999\u001b[0m     \"\"\"\n\u001b[1;32m   1000\u001b[0m     return moving_averages.assign_moving_average(\n\u001b[0;32m-> 1001\u001b[0;31m         x, value, momentum, zero_debias=True)\n\u001b[0m\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/moving_averages.py\u001b[0m in \u001b[0;36massign_moving_average\u001b[0;34m(variable, value, decay, zero_debias, name)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mdecay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mzero_debias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mupdate_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_zero_debias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mupdate_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/moving_averages.py\u001b[0m in \u001b[0;36m_zero_debias\u001b[0;34m(unbiased_var, value, decay)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mlocal_step_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m       biased_var = variable_scope.get_variable(\n\u001b[0;32m--> 180\u001b[0;31m           \"biased\", initializer=biased_initializer, trainable=False)\n\u001b[0m\u001b[1;32m    181\u001b[0m       local_step = variable_scope.get_variable(\n\u001b[1;32m    182\u001b[0m           \u001b[0;34m\"local_step\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1047\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1050\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1051\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    946\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    651\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 653\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    654\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable batch_normalization_1/moving_mean/biased already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/Users/shreyashkawalkar/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 1001, in moving_average_update\n    x, value, momentum, zero_debias=True)\n  File \"/Users/shreyashkawalkar/anaconda3/lib/python3.6/site-packages/keras/layers/normalization.py\", line 185, in call\n    self.momentum),\n  File \"/Users/shreyashkawalkar/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() \n",
    "sequence = Input(shape=[None, 64, 64, 3])\n",
    "conv = TimeDistributed(cnnmodel)(sequence)\n",
    "dense_512_c = TimeDistributed(Dense(6,activation='softmax'))(conv)\n",
    "lstm = LSTM(output_dim=6, return_sequences=True)(dense_512_c)\n",
    "model = Model(inputs= sequence, outputs=lstm)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#up_weights = update_weights_intensity(weights)\n",
    "#lstm.set_weights(up_weights)\n",
    "#here completes of E1 and E2\n",
    "#model.fit(xtr,ytr, shuffle = False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
