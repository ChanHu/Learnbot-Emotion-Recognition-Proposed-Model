{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.models import Sequential, Layer\n",
    "from keras.layers import Dense, MaxPool2D, Dropout,MaxPool2D, Activation,BatchNormalization, InputLayer, Embedding, Flatten, Conv2D,concatenate, MaxPooling2D, Input, TimeDistributed\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import keras.utils\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Flatten, LSTM\n",
    "import numpy as np\n",
    "from keras import objectives\n",
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "(6, 10, 64, 64, 3)\n",
      "(6,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'channels_last'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = np.random.rand(10,64,64,3).reshape(-1,64,64,3)\n",
    "s2 = np.random.rand(10,64,64,3).reshape(-1,64,64,3)\n",
    "s3 = np.random.rand(10,64,64,3).reshape(-1,64,64,3)\n",
    "s4 = np.random.rand(10,64,64,3).reshape(-1,64,64,3)\n",
    "s5 = np.random.rand(10,64,64,3).reshape(-1,64,64,3)\n",
    "s6 = np.random.rand(10,64,64,3).reshape(-1,64,64,3)\n",
    "data = np.array([s1,s2,s3,s4,s5,s6]).reshape(-1,10,64,64,3)\n",
    "label = np.array([0,1,2,3,4,5]).reshape(-1)\n",
    "print(s1.shape)\n",
    "print(data.shape)\n",
    "print(label.shape)\n",
    "label = keras.utils.to_categorical(label, num_classes=None)\n",
    "K.set_image_data_format('channels_last')\n",
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.random.rand(10,512).reshape(-1,10,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"input_16:0\", shape=(?, 10, 512), dtype=float32) at layer \"input_16\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-413-14c0779b0701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mh_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lstm_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#E2 Objective Covered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m   1807\u001b[0m                                 \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                                 \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m                                 str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1810\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m                         \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"input_16:0\", shape=(?, 10, 512), dtype=float32) at layer \"input_16\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "inp = Input(shape = (10,512))\n",
    "h_layer = LSTM(512, return_sequences=True, dropout=0.2, name='lstm_1')(inp)\n",
    "mod = Model(x, h_layer)\n",
    "mod.compile(loss= loss_func, optimizer = 'adam', metrics=['accuracy'])#E2 Objective Covered\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRegularization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomRegularization, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self ,x ,mask=None):\n",
    "        inp = InputLayer(input_shape = (10,512))\n",
    "        h_layer = LSTM(512, return_sequences=True, dropout=0.2, name='lstm_1')(inp)\n",
    "        mod = Model( x, h_layer)\n",
    "        mod.compile(loss= loss_func, optimizer = 'adam', metrics=['accuracy'])#E2 Objective Covered\n",
    "        return h_layer\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        print(input_shape)\n",
    "        return ((None, 10, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def min_max_norm(mat):\n",
    "    min_m = min(mat)\n",
    "    max_m = max(mat)\n",
    "    normalized =[((x-min_m)/(max_m-min_m)) for x in mat];\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-416-496f46d5b5e7>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-416-496f46d5b5e7>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    #     return nrm_arr\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def cossy(x_mat):\n",
    "#     x_mat = keras.backend.get_value(x_mat)\n",
    "#     k_var = K.placeholder(shape=(10,1), dtype='float32')\n",
    "#     print((x_mat).eval(session=k.get_session()))\n",
    "#     apex_vec = x_mat[-1][:]\n",
    "#     print(type(apex_vec))\n",
    "#     dotting = x_mat.dot(apex_vec)\n",
    "#     dotting = np.array([col.dot(apex_vec) for col in x_mat])\n",
    "#     print(keras.backend.int_shape(x_mat))\n",
    "#     dotting = np.array([K.dot(x_mat[i][:], apex_vec) for i in range(10)])\n",
    "#     print(dotting)\n",
    "#     matrix_norms = np.linalg.norm(x_mat, axis=1)\n",
    "#     vector_norm = np.linalg.norm(apex_vec)\n",
    "#     matrix_vector_norms = np.multiply(matrix_norms, vector_norm)\n",
    "#     coss_mat = np.divide(dotting, matrix_vector_norms)\n",
    "#     print(\"coss_mat: \",coss_mat.shape)\n",
    "#     nrm_arr = np.array(min_max_norm(coss_mat))\n",
    "#     print(\"nrm_arr: \",nrm_arr.shape)\n",
    "#     return nrm_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossimilasrity(x):\n",
    "#     tt = K.variable(np.random.rand(10,512).astype('f').reshape(10,512))\n",
    "#     tt = np.random.rand(10,512).astype('f').reshape(10,512)\n",
    "#     print(tt.shape)\n",
    "#     y = keras.layers.Lambda(cossy, output_shape = [10] )(x)\n",
    "    k_var = K.placeholder(shape=(10,512), dtype='float32', name = 'lstm_1')\n",
    "    tt = (x).eval(session=k.get_session())\n",
    "    print(type(tt))\n",
    "    y = cossy(tt)\n",
    "    print(\"y: \",y)\n",
    "#     return y #np.random.rand(10).astype('f').reshape(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(y_true, y_pred):\n",
    "    print(\"loss_func\")\n",
    "    y_true = cossimilasrity(y_true)\n",
    "    y_pred = cossimilasrity(y_pred)\n",
    "    print(y_true.shape)\n",
    "    print(y_pred.shape)\n",
    "    loss = objectives.mean_squared_error(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1111 (TimeD (None, 10, 60, 60, 64)    4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_445 (Bat (None, 10, 60, 60, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_1112 (TimeD (None, 10, 60, 60, 64)    36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_446 (Bat (None, 10, 60, 60, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_1113 (TimeD (None, 10, 30, 30, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1114 (TimeD (None, 10, 30, 30, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1115 (TimeD (None, 10, 30, 30, 32)    18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_447 (Bat (None, 10, 30, 30, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_1116 (TimeD (None, 10, 30, 30, 32)    9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_448 (Bat (None, 10, 30, 30, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_1117 (TimeD (None, 10, 15, 15, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1118 (TimeD (None, 10, 15, 15, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1119 (TimeD (None, 10, 7200)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1120 (TimeD (None, 10, 512)           3686912   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 6)                 12456     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 3,769,640\n",
      "Trainable params: 3,769,256\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.set_learning_phase(1)\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(64, 5, data_format=\"channels_last\", kernel_initializer=\"he_normal\", activation='relu'), input_shape=s1.shape))\n",
    "#model.add(TimeDistributed())\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3),  padding='same', activation='relu')))\n",
    "# model.add(TimeDistributed())\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3),padding='same',activation='relu')))\n",
    "# model.add(TimeDistributed())\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3),padding='same',activation='relu')))\n",
    "# model.add(TimeDistributed())\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(TimeDistributed(Dense(512)))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#freeze\n",
    "# model.add(CustomRegularization())\n",
    "#freeze\n",
    "#model.add()\n",
    "model.add(LSTM(6, return_sequences=False, dropout=0.2, name='lstm_2'))#E1 Objective Covered\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])#'categorical_crossentropy'\n",
    "#compile E3 layerlstm\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(data, label, batch_size=1, epochs = 1)\n",
    "# score, acc = model.evaluate(data, label)\n",
    "# print('Test score:', score)\n",
    "# print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_32 (Conv2D)           (None, 60, 60, 64)        4864      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 60, 60, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 57, 57, 64)        65600     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 57, 57, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 26, 26, 32)        18464     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               2359808   \n",
      "=================================================================\n",
      "Total params: 2,458,112\n",
      "Trainable params: 2,458,048\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#cnn mian seperate import\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(Conv2D(64, 5, data_format=\"channels_last\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape=(64, 64, 3)))\n",
    "#cnnmodel.add(BatchNormalization())\n",
    "cnnmodel.add(Activation(\"relu\"))\n",
    "\n",
    "cnnmodel.add(Conv2D(64, 4))\n",
    "#cnnmodel.add(BatchNormalization())\n",
    "cnnmodel.add(Activation(\"relu\"))\n",
    "cnnmodel.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "cnnmodel.add(Dropout(0.5))\n",
    "\n",
    "cnnmodel.add(Conv2D(32, 3))\n",
    "#cnnmodel.add(BatchNormalization())\n",
    "cnnmodel.add(Activation(\"relu\"))\n",
    "\n",
    "cnnmodel.add(Conv2D(32, 3))\n",
    "cnnmodel.add(keras.layers.normalization.BatchNormalization())#BatchNormalization())\n",
    "cnnmodel.add(Activation(\"relu\"))\n",
    "\n",
    "cnnmodel.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "cnnmodel.add(Dropout(0.5))\n",
    "\n",
    "cnnmodel.add(Flatten())\n",
    "cnnmodel.add(Dense(512))\n",
    "cnnmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnnmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_8 (TimeDist (None, 1000, 60, 60, 64)  4864      \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 1000, 58, 58, 32)  18464     \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 1000, 29, 29, 32)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 1000, 29, 29, 64)  18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 1000, 29, 29, 64)  36928     \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 1000, 14, 14, 64)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 1000, 14, 14, 128) 73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 1000, 14, 14, 128) 147584    \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 1000, 7, 7, 128)   0         \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 1000, 7, 7, 256)   295168    \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 1000, 7, 7, 256)   590080    \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 1000, 3, 3, 256)   0         \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 1000, 3, 3, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 1000, 3, 3, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "time_distributed_22 (TimeDis (None, 1000, 1, 1, 512)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1000, 1, 1, 512)   2048      \n",
      "_________________________________________________________________\n",
      "time_distributed_23 (TimeDis (None, 1000, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000, 512)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1000, 256)         787456    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000, 6)           1542      \n",
      "=================================================================\n",
      "Total params: 5,516,454\n",
      "Trainable params: 5,515,430\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## approach 2\n",
    "model = Sequential()\n",
    "sequence = Input(shape=[None, 64, 64, 3])\n",
    "model.add(TimeDistributed(Conv2D(64, 5, data_format=\"channels_last\", kernel_initializer=\"he_normal\"), input_shape=(1000, 64, 64, 3)))\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3),  padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(128, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(128, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(256, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(256, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(512, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(512, (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(256, return_sequences=True, dropout=0.5))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
